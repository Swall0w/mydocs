<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml" lang="ja">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>1. 機械学習のまとめ &#8212; MyOutput 0.0.1 ドキュメント</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/translations.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="検索" href="search.html" />
    <link rel="next" title="2. 数学" href="learned_math.html" />
    <link rel="prev" title="Welcome to MyOutput’s documentation!" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="id1">
<h1>1. 機械学習のまとめ<a class="headerlink" href="#id1" title="このヘッドラインへのパーマリンク">¶</a></h1>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">著者:</th><td class="field-body">Masato</td>
</tr>
</tbody>
</table>
<div class="section" id="id2">
<h2>1.1. 概要<a class="headerlink" href="#id2" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>機械学習に必要な知識についてまとめてあります。参考文献葉は</p>
<ol class="arabic simple">
<li>Christopher M. Bishop &#8220;Pattern Recognition and Machine Learning&#8221;</li>
<li>機械学習プロフェッショナルシリーズ</li>
<li>人工知能学会「深層学習」</li>
</ol>
<p>からまとめています。現在は深層学習に向けた知識をまとめるので、主に取り扱っているのはニューラルネットワークです。</p>
</div>
<div class="section" id="id3">
<h2>1.2. 機械学習とは<a class="headerlink" href="#id3" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id4">
<h2>1.3. 確率分布<a class="headerlink" href="#id4" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id5">
<h2>1.4. 線形回帰モデル<a class="headerlink" href="#id5" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id6">
<h2>1.5. 線形識別モデル<a class="headerlink" href="#id6" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id7">
<h2>1.6. ニューラルネットワーク<a class="headerlink" href="#id7" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="math">
<p><img src="_images/math/daa31072e676417d1c4ce28656e83ef0f54120ea.png" alt="(a + b)^2 = a^2 + 2ab + b^2

(a - b)^2 = a^2 - 2ab + b^2

\bm{A}"/></p>
</div><div class="section" id="id8">
<h3>順伝搬型ネットワーク<a class="headerlink" href="#id8" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="section" id="id9">
<h4>ユニットの出力<a class="headerlink" href="#id9" title="このヘッドラインへのパーマリンク">¶</a></h4>
<blockquote>
<div>順伝搬型ネットワーク(feedforward neural network)は、層状に並べたユニットが隣接層間でのみ結合した構造を持ち、情報が入力側から出力側に一方向にのみ伝搬するニューラルネットワークである。文権によっては、
多層パーセプトロン(Multi-layer Perceptron)と呼ばれる。
　ネットワークを構成するユニットは、複数の入力を受け取り、1つの出力を計算します。例えば、4つの入力 <img class="math" src="_images/math/bf8e67ffbfc0d517f37e0e469a3cdcf3f41784e6.png" alt="x_{1}, x_{2}, x_{3} , x_{4}"/> を受け取った場合の総入力 <cite>u</cite> は、</div></blockquote>
<div class="math">
<p><img src="_images/math/c1586ea97a9c1f458759c9cba8c22c58b5dbc4f8.png" alt="u = w_{1}x_{1} + w_{2}x_{2} +  w_{3}x_{3} +  w_{4}x_{4} + b"/></p>
</div><p>のように各入力にそれぞれ異なる値の重み(weight) <img class="math" src="_images/math/9998061a5071c7715cf1044c5b6db6ce871672da.png" alt="w_{1}, w_{2}, w_{3} , w_{4}"/> を掛けた値を加算し、これにバイアス(bias)を足し合わせたものになる。ユニットの出力 <img class="math" src="_images/math/683f2dd9129a91d21aaf1c04afa6f78b39d4cb0a.png" alt="z"/> は、総入力 <img class="math" src="_images/math/92e3f44c99a016c5238a790c96f12fd46aa93c4a.png" alt="u"/> に対する活性化関数
(activation function)と呼ばれる関数 <img class="math" src="_images/math/875eb40014526135383caa89fd500ae40a835f56.png" alt="f"/> の出力となる。</p>
<div class="math">
<p><img src="_images/math/e9d09fab94cf796928c7603cf48be096e9859c7a.png" alt="z = f(u)"/></p>
</div><img alt="_images/DeepLearningch2_1.png" src="_images/DeepLearningch2_1.png" />
<p>順伝搬型ネットワークでは、下図のようになり、このようなユニットが層状に並べられ、層間でのみ結合を持つ。前段の層のユニットの出力が後段の層のユニットの出力になるという形で、この結合を通じて信号は前から後ろに一方向に伝搬する。
同図のネットワークでは、後段の3つのユニット( <img class="math" src="_images/math/fefeb3cc645203cb10b57c8a970378e80e6ba23a.png" alt="j = 1,2,3"/> )はそれぞれ、前段の層の4つのユニット( <img class="math" src="_images/math/6e1f0bf5e9fb01d71c506c5e72d4941000a4eb42.png" alt="i = 1,2,3,4"/> )からの出力 <img class="math" src="_images/math/8fd0bb771160498fb58bf10e5759ebf926ac9e12.png" alt="x_1, x_2, x_3, x_4"/> を入力として受け取る。ユニットの結合は3x4=12本ありが、
その1つ1つの結合に異なる重み <img class="math" src="_images/math/8250d6b5f7158b309508255642586d1ddb618470.png" alt="w_{ji}"/> が与えられている。次式は3つのユニットが受け取る入力はそれぞれ、</p>
<div class="math">
<p><img src="_images/math/30ed8fb59d13a024df76b3deab82f019216331b0.png" alt="u_1 = w_{11}x_{1} + w_{12}x_{2} +  w_{13}x_{3} +  w_{14}x_{4} + b_1

u_2 = w_{21}x_{1} + w_{22}x_{2} +  w_{23}x_{3} +  w_{24}x_{4} + b_2

u_3 = w_{31}x_{1} + w_{32}x_{2} +  w_{33}x_{3} +  w_{34}x_{4} + b_3"/></p>
</div><p>で計算され、これらに活性化関数を適用したものが出力となる。</p>
<div class="math">
<p><img src="_images/math/9071db0cf96ab3d85ed2a74f716e14c2f896a8d5.png" alt="z_j = f(u_j) (j = 1,2,3)"/></p>
</div><img alt="_images/DeepLearningch2_2.png" src="_images/DeepLearningch2_2.png" />
<p>第1層のユニットを <img class="math" src="_images/math/3a6cde09b922f370f32d40d8ccc5d2a367a0c0f2.png" alt="i = 1, ..., I,"/>
第2層のユニットを <img class="math" src="_images/math/f056d57636d6bd76d1621ace11c6f8dc32259d5d.png" alt="j = 1, ..., J,"/> で表すと、
第1層のユニットの出力から第2層のユニットへの出力が決まるまでの計算は次の様になる。 <img class="math" src="_images/math/724def13eaa72345614d6ec7b48727306356f3cc.png" alt="( j = 1, ..., J )"/> .</p>
<div class="math">
<p><img src="_images/math/c03cdd71aa28058b06389599f425ddeb3ae6d159.png" alt="u_j = \sum_{i=1}^n w_{ji}x_{i} + b_j

z_j = f(u_j)"/></p>
</div><p>ベクトル化すると、</p>
<div class="math">
<p><span class="math">\vector[u] = \vector[Wx] + \vector[b]

\vector[z] = f(\vector[u])</span></p>
</div></div>
</div>
<div class="section" id="id10">
<h3>確率的勾配降下法<a class="headerlink" href="#id10" title="このヘッドラインへのパーマリンク">¶</a></h3>
</div>
<div class="section" id="id11">
<h3>誤差逆伝搬<a class="headerlink" href="#id11" title="このヘッドラインへのパーマリンク">¶</a></h3>
</div>
<div class="section" id="id12">
<h3>事前学習(自己符号化器)<a class="headerlink" href="#id12" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="section" id="autoencoder">
<h4>AutoEncoder<a class="headerlink" href="#autoencoder" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>AutoEncoder(AE)は、出力データが入力データをそのまま再現する3層のNNである。そのため、入力層から中間層への変換はエンコード、中間層から出力層への変換はデコードと呼ばれる。中間層のノード数は入力データの次元数以上だと、
エンコードが恒等関数になるので、中間層のノード数は入力データの次元よりも小さくする。つまり、中間層の出力は入力データを次元圧縮したものと表現している。</p>
<p>chainerによるIrisデータセットの次元圧縮のコードがこちら</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">chainer</span>
<span class="kn">from</span> <span class="nn">chainer</span> <span class="k">import</span> <span class="n">cuda</span><span class="p">,</span> <span class="n">Function</span><span class="p">,</span> <span class="n">gradient_check</span><span class="p">,</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">serializers</span><span class="p">,</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">chainer</span> <span class="k">import</span> <span class="n">Link</span><span class="p">,</span> <span class="n">Chain</span><span class="p">,</span> <span class="n">ChainList</span>
<span class="kn">import</span> <span class="nn">chainer.functions</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">chainer.links</span> <span class="k">as</span> <span class="nn">L</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">xtrain</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MyAE</span><span class="p">(</span><span class="n">Chain</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
            <span class="n">l1</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">l2</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">bv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fwd</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">bv</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">fwd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">fv</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">bv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">fv</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">bv</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyAE</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
    <span class="n">sffindx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span> <span class="p">,</span><span class="n">bs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">xtrain</span><span class="p">[</span><span class="n">sffindx</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="n">bs</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">bs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="k">else</span> <span class="n">n</span><span class="p">]])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zerograds</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span>
<span class="n">yt</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">ans</span> <span class="o">=</span> <span class="n">yt</span><span class="o">.</span><span class="n">data</span>
<span class="n">ansx1</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ansy1</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ansx2</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ansy2</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ansx3</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">150</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ansy3</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">150</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ansx1</span><span class="p">,</span><span class="n">ansy1</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;^&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ansx2</span><span class="p">,</span><span class="n">ansy2</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ansx3</span><span class="p">,</span><span class="n">ansy3</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>これでは、ChainクラスのMyAEの損失関数の部分で教師信号が入力信号が入力データ <cite>x</cite> になっているのがポイントである。</p>
</div>
<div class="section" id="id13">
<h4>AutoEncoder<a class="headerlink" href="#id13" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>AEでは中間層のノード数が入力信号のノード数（入力ベクトルの次元数）よりも多いと恒等関数になってしまう問題があり、次元圧縮として使うなら問題はないが、次元悪sy区とは逆に入力ベクトルを
より高次元の空間に射影したい場合は使うことができない。SVMのように高次元空間に射影し、その空間では識別が線形で行えるなどの利点を求める場合、Denoising AutoEncoder(DAE)を用いることで、
高次元に射影することも可能である。また、高次元への射影ではなく、通常の次元圧縮にもDAEを使うことができ、AEの改良版となる。しかし、理論的に示すのは難しい。
ノイズを入れることで確認出来る。
<a class="reference external" href="https://github.com/bohemian916/deeplearning_tool/blob/master/increase_picture.py">https://github.com/bohemian916/deeplearning_tool/blob/master/increase_picture.py</a></p>
</div>
</div>
<div class="section" id="id14">
<h3>畳み込みニューラルネット<a class="headerlink" href="#id14" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="section" id="id15">
<h4>画像認識<a class="headerlink" href="#id15" title="このヘッドラインへのパーマリンク">¶</a></h4>
</div>
</div>
<div class="section" id="id16">
<h3>再帰型ニューラルネット<a class="headerlink" href="#id16" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="section" id="id17">
<h4>音声認識<a class="headerlink" href="#id17" title="このヘッドラインへのパーマリンク">¶</a></h4>
</div>
<div class="section" id="id18">
<h4>自然言語処理<a class="headerlink" href="#id18" title="このヘッドラインへのパーマリンク">¶</a></h4>
</div>
</div>
<div class="section" id="id19">
<h3>ボルツマンマシン<a class="headerlink" href="#id19" title="このヘッドラインへのパーマリンク">¶</a></h3>
</div>
</div>
<div class="section" id="id20">
<h2>1.7. カーネル法<a class="headerlink" href="#id20" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id21">
<h2>1.8. 疎な解を持つカーネルマシン<a class="headerlink" href="#id21" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id22">
<h2>1.9. グラフィカルモデル<a class="headerlink" href="#id22" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="em">
<h2>1.10. 混合モデルとEM<a class="headerlink" href="#em" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id23">
<h2>1.11. 近似推論法<a class="headerlink" href="#id23" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id24">
<h2>1.12. サンプリング法<a class="headerlink" href="#id24" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここではマルコフ連鎖モンテカルロ法などについて述べられる。</p>
</div>
<div class="section" id="id25">
<h2>1.13. 連続潜在変数<a class="headerlink" href="#id25" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id26">
<h2>1.14. 系列データ<a class="headerlink" href="#id26" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id27">
<h2>1.15. モデルの結合<a class="headerlink" href="#id27" title="このヘッドラインへのパーマリンク">¶</a></h2>
</div>
<div class="section" id="id28">
<h2>1.16. 実践<a class="headerlink" href="#id28" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="section" id="caffe">
<h3>Caffeの使い方<a class="headerlink" href="#caffe" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ネットワークの基本的な攻勢はlayerを作成し、それらをblobと命名して結合していく。このlayerの作成とblobの結合は、prototxtという設定ファイルに記述していく。Caffeによる学習と評価には、3つのファイルが必要である。</p>
<ul class="simple">
<li>train_test.prototxt       : ネットワーク攻勢と学習データセット</li>
<li>solver.prototxt           : 学習パラメータ</li>
<li>deploy.prototxt           : 入力データ情報とネットワーク構成</li>
</ul>
<p>これらの名前は上記と同じなくていい。</p>
<p><a class="reference external" href="https://gist.github.com/rezoo/a1c8d1459b222fc5658f">https://gist.github.com/rezoo/a1c8d1459b222fc5658f</a>
構築方法(AMI)
<a class="reference external" href="http://jnory.hatenablog.com/entry/archives/137">http://jnory.hatenablog.com/entry/archives/137</a></p>
</div>
<div class="section" id="id29">
<h3>データ拡張<a class="headerlink" href="#id29" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>今あるサンプルをもとにバリエーションを増やして枚数を増やすデータ拡張デス。データ拡張は、サンプルに対して平行移動や回転変動を加えたり、鏡面反転させたりして、返歌を加えたサンプルを作成する方法です。
加える変化は上記以外にも、ランダムノイズやボケを加えることもある。平行移動や回転変動を加える場合、対象とするサンプルによって画像外はにはみ出すこともある。
手書き文字認識などのように、形状変化が生じる問題を対象賭する場合、形状に変形を加えてデータ拡張することもある。形状変化させる方法としては、弾性変形(elastic distortion)がある。
[P. Y. Simard, D.Steinkraus, and J. C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In International Conference on Document Analysis and Recognition(ICDAR), pp.958-962,2003.]
弾性変形は、バイナリニアやバイキューブの補間方法を利用する。</p>
</div>
<div class="section" id="id30">
<h3>前処理<a class="headerlink" href="#id30" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>入力データから特徴変換せず、入力データをそのまま与えるのがディープラーニングでは一般的である。しかし、クラス内の変動が大きい場合は、変動を少しでも抑えるような前処理をすることが多い。前処理の代表例としては以下のものがある。</p>
<ul class="simple">
<li>平均値除去</li>
<li>正規化</li>
<li>白色化</li>
</ul>
<div class="section" id="id31">
<h4>平均値除去<a class="headerlink" href="#id31" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>平均値除去は大規模な一般物体認識で用いられる方法であり、学習する画像すべての平均画像を以下のように求める。</p>
<p>学習サンプルは平均画像を引いた差分画像xを入力データとする。
これにより、各要素の平均が0になるようにし、全体的に明るさの変動を抑えた様な画像になっている。</p>
</div>
<div class="section" id="id32">
<h4>正規化<a class="headerlink" href="#id32" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>正規化(normalization)は、各サンプルの平均と分散を合わせる前処理です。平均値除去は各要素の平均が0になるようにしているが、正規化ではさらに各要素のばらつきを合わせるように分散を1にする。
まず、各要素の標準偏差 <img class="math" src="_images/math/8b7987a1b64e009719daaf744c922bf934e69583.png" alt="\sigma_i"/> を求める。</p>
<p>平均値除去の後、求めた標準偏差で割る。
これにより、各要素が平均0、分散1になる。平均値除去を行った場合よりも画像間の明るさのばらつきが小さくなる。</p>
</div>
<div class="section" id="id33">
<h4>白色化<a class="headerlink" href="#id33" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>白色化(whitening)は、要素間の相関をなくす方法である。これにより、相関の低い要素の情報が残り、画像の場合は、エッジが強調されたような画像になる。まず、平均値を行い、平均が0になるようにする。これを</p>
<p>すべての学習サンプルについて、平均が0になるようにしたら、それを</p>
<p>正規化処理とゼロ位相白色化を行ったものは、直流成分のような相関の高い画素の情報は消え、相関の低い画素、すなわちエッジ成分が残っている。特徴的な画像にすることによって認識性能を向上させることが出来る。</p>
</div>
</div>
<div class="section" id="id34">
<h3>活性化関数<a class="headerlink" href="#id34" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ニューラルネットワークでは活性化関数としてシグモイド関数が主に利用されていたが、ディープラーニングでReLUが提案されて以降、新たな活性化関数やReLUの派生が提案されている。</p>
<ul class="simple">
<li>マックスアウト</li>
<li>Leaky ReLU</li>
<li>Parametric ReLU(PReLU)</li>
<li>Randomized leaky Rectified Linear Units (RReLU)</li>
</ul>
<div class="section" id="id35">
<h4>マックスアウト<a class="headerlink" href="#id35" title="このヘッドラインへのパーマリンク">¶</a></h4>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">目次</a></h3>
  <ul>
<li><a class="reference internal" href="#">1. 機械学習のまとめ</a><ul>
<li><a class="reference internal" href="#id2">1.1. 概要</a></li>
<li><a class="reference internal" href="#id3">1.2. 機械学習とは</a></li>
<li><a class="reference internal" href="#id4">1.3. 確率分布</a></li>
<li><a class="reference internal" href="#id5">1.4. 線形回帰モデル</a></li>
<li><a class="reference internal" href="#id6">1.5. 線形識別モデル</a></li>
<li><a class="reference internal" href="#id7">1.6. ニューラルネットワーク</a><ul>
<li><a class="reference internal" href="#id8">順伝搬型ネットワーク</a><ul>
<li><a class="reference internal" href="#id9">ユニットの出力</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10">確率的勾配降下法</a></li>
<li><a class="reference internal" href="#id11">誤差逆伝搬</a></li>
<li><a class="reference internal" href="#id12">事前学習(自己符号化器)</a><ul>
<li><a class="reference internal" href="#autoencoder">AutoEncoder</a></li>
<li><a class="reference internal" href="#id13">AutoEncoder</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id14">畳み込みニューラルネット</a><ul>
<li><a class="reference internal" href="#id15">画像認識</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id16">再帰型ニューラルネット</a><ul>
<li><a class="reference internal" href="#id17">音声認識</a></li>
<li><a class="reference internal" href="#id18">自然言語処理</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id19">ボルツマンマシン</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id20">1.7. カーネル法</a></li>
<li><a class="reference internal" href="#id21">1.8. 疎な解を持つカーネルマシン</a></li>
<li><a class="reference internal" href="#id22">1.9. グラフィカルモデル</a></li>
<li><a class="reference internal" href="#em">1.10. 混合モデルとEM</a></li>
<li><a class="reference internal" href="#id23">1.11. 近似推論法</a></li>
<li><a class="reference internal" href="#id24">1.12. サンプリング法</a></li>
<li><a class="reference internal" href="#id25">1.13. 連続潜在変数</a></li>
<li><a class="reference internal" href="#id26">1.14. 系列データ</a></li>
<li><a class="reference internal" href="#id27">1.15. モデルの結合</a></li>
<li><a class="reference internal" href="#id28">1.16. 実践</a><ul>
<li><a class="reference internal" href="#caffe">Caffeの使い方</a></li>
<li><a class="reference internal" href="#id29">データ拡張</a></li>
<li><a class="reference internal" href="#id30">前処理</a><ul>
<li><a class="reference internal" href="#id31">平均値除去</a></li>
<li><a class="reference internal" href="#id32">正規化</a></li>
<li><a class="reference internal" href="#id33">白色化</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id34">活性化関数</a><ul>
<li><a class="reference internal" href="#id35">マックスアウト</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="前の章へ">Welcome to MyOutput&#8217;s documentation!</a></li>
      <li>Next: <a href="learned_math.html" title="次の章へ">2. 数学</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>このページ</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/learned_machine_learning.rst.txt"
            rel="nofollow">ソースコードを表示</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>クイック検索</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="検索" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Masato.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.5.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="_sources/learned_machine_learning.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>